{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aux_RecAttModel_Multitask_Aug_Data_Villa_Cattan_RECSYS_champ_type.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a705d3b71b5d4fb587b3bb1fb38161fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_abfceea38af9444b8da09122eb0c867d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0dfe065d9c0c468389f75dd74a9a11ac",
              "IPY_MODEL_741dabfff47941f3b290d4ad4cb6be12"
            ]
          }
        },
        "abfceea38af9444b8da09122eb0c867d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "0dfe065d9c0c468389f75dd74a9a11ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_503e7ca9191948a4bcf6cc64a8862820",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_249092a9ce0940218d291fe75cf35f3e"
          }
        },
        "741dabfff47941f3b290d4ad4cb6be12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_828e6e2c56f3456cb016bf7c8b701ba8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1.0 [00:05&lt;00:00,  2.86s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7540c343bf9f461a84c157f84d529cd9"
          }
        },
        "503e7ca9191948a4bcf6cc64a8862820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "249092a9ce0940218d291fe75cf35f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "828e6e2c56f3456cb016bf7c8b701ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7540c343bf9f461a84c157f84d529cd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c43e576730a940c28fa78d49e95e7165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b3d8f9b86d0d47ae8c51b8f2eb202aab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_82170aabdef246edbf668bb1cdf4a5e3",
              "IPY_MODEL_43a58bf6d9ab454095f8f5f30f10cdca"
            ]
          }
        },
        "b3d8f9b86d0d47ae8c51b8f2eb202aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "82170aabdef246edbf668bb1cdf4a5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f80b50d773b240a091c6c3bdf7961924",
            "_dom_classes": [],
            "description": "Epoch 1:  10%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1577,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 160,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8a352088b8d4ed896bf8a206ecc024e"
          }
        },
        "43a58bf6d9ab454095f8f5f30f10cdca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e151daf92e84c369ee90e8ded7a24f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160/1577 [06:58&lt;1:01:48,  2.62s/it, loss=0.485, v_num=8dcde95206ec45daac4cc6657844b03d, train_loss=0.0916, train_loss_aux=1.97, train_prec_avg=0.405, total_loss_train=0.485, train_loss_avg=0.117]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b0b2240357743c4b8285ce6017638c4"
          }
        },
        "f80b50d773b240a091c6c3bdf7961924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8a352088b8d4ed896bf8a206ecc024e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e151daf92e84c369ee90e8ded7a24f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b0b2240357743c4b8285ce6017638c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoLSVVIBCwLm",
        "colab_type": "text"
      },
      "source": [
        "# Interpretable Contextual Team-aware Item Recommendation: Application in Multiplayer Online Battle Arena Games\n",
        "*Andres Villa, Vladimir Araujo, Francisca Cattan*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8_YV_PIDR97",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook contains the code of the proposed model. It is composed of 8 main stages:\n",
        "\n",
        "1. Connect to gDrive\n",
        "2. Dataset and Transformations\n",
        "3. Model\n",
        "4. Logger and Checkpointer\n",
        "5. Metrics\n",
        "6. Training and evaluation loop\n",
        "7. Config file\n",
        "8. Training and evaluation executor\n",
        "9. Obtain the role and id of each champion in each match\n",
        "10. Load the attention weights\n",
        "11. Draw the attention map\n",
        "\n",
        "*This notebook can be run in it's entirety. The final cell executes the training and validation of the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYxhCKYPbBT2",
        "colab_type": "toc"
      },
      "source": [
        ">[Main Model - Project Title](#scrollTo=uoLSVVIBCwLm)\n",
        "\n",
        ">[Introduction](#scrollTo=t8_YV_PIDR97)\n",
        "\n",
        ">[Install all the dependencies](#scrollTo=etkQTYydGkFM)\n",
        "\n",
        ">[Import the dependencies](#scrollTo=S0YvGjijGxET)\n",
        "\n",
        ">[Connect to gDrive](#scrollTo=pfDyM4E7G4L2)\n",
        "\n",
        ">[Dataset and Transformations](#scrollTo=h9MDWroJSkhM)\n",
        "\n",
        ">[Model](#scrollTo=UIm1_KUCUNB0)\n",
        "\n",
        ">>[Transformer encoder modified to obtain the attention weights](#scrollTo=qr3TZbrnUg2H)\n",
        "\n",
        ">>[Auxiliary Task Classes](#scrollTo=pwRy106QU6sH)\n",
        "\n",
        ">>[Main Class of the proposed model](#scrollTo=9AlU_u42VG8A)\n",
        "\n",
        ">[Logger and Checkpointer](#scrollTo=rwYoKWcsVqex)\n",
        "\n",
        ">[Metrics](#scrollTo=5ktMqAUMWeEz)\n",
        "\n",
        ">[Training and evaluation loop](#scrollTo=WDA0GHysW4vX)\n",
        "\n",
        ">[Config file](#scrollTo=CyRfaqN8XvYi)\n",
        "\n",
        ">[Training and evaluation executor](#scrollTo=IVtKoVTcYDS1)\n",
        "\n",
        ">[T-test](#scrollTo=D2TPs5U3vv7m)\n",
        "\n",
        ">[Obtain the role and id of each champion in each match](#scrollTo=sFtaUCU5T8fl)\n",
        "\n",
        ">[Load the attention weights](#scrollTo=VINfHm76U1vz)\n",
        "\n",
        ">[Draw the attention map](#scrollTo=SvhQCEzcU6x_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkQTYydGkFM",
        "colab_type": "text"
      },
      "source": [
        "# Install all the dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH0huCgYRJou",
        "colab_type": "text"
      },
      "source": [
        "Install all the libraries neccesary to run the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqG6vM9tqER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "1450ea6d-a49f-4586-b3a9-6c111b197efa"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 28 06:40:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTd06UwscDsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca753b75-1078-420f-cdfb-b3e5ee505b97"
      },
      "source": [
        "!pip install git+git://github.com/williamFalcon/pytorch-lightning.git@master --upgrade"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/williamFalcon/pytorch-lightning.git@master\n",
            "  Cloning git://github.com/williamFalcon/pytorch-lightning.git (to revision master) to /tmp/pip-req-build-j84m_1zy\n",
            "  Running command git clone -q git://github.com/williamFalcon/pytorch-lightning.git /tmp/pip-req-build-j84m_1zy\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0rc2) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0rc2) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0rc2) (1.5.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0rc2) (2.2.2)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.9.0rc2) (3.1.0)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.9.0rc2-cp36-none-any.whl size=353828 sha256=30b73a303ccd241770a24f1984519ad7e086144a4db7285f7b8166e4de330d64\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlmk53yu/wheels/02/e9/33/ecf2ab0b937f47c530a3d24222ca1a784412a0c7d490195c5f\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: future, PyYAML\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=15e732369ebb372a11250e7c9d0e57f7af49cc24c3e76854d4126655f02df3b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=085fe27c9be3cbcd42d6336ee010929d00e36263ea5e88a9613aeb9fbf6e1b49\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built future PyYAML\n",
            "Installing collected packages: future, PyYAML, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 future-0.18.2 pytorch-lightning-0.9.0rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwD1P0lHVaLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "4d2b24ba-90cd-4304-cb42-f09c697a3827"
      },
      "source": [
        "!pip install comet_ml==3.0.2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting comet_ml==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/c6/fac88f43f2aa61a09fee4ffb769c73fe93fe7de75764246e70967d31da09/comet_ml-3.0.2-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 2.9MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.9MB/s \n",
            "\u001b[?25hCollecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml==3.0.2) (2.23.0)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/1e/52f4effa64a447c4ec0fb71222799e2ac32c55b4b6c1725fccdf6123146e/wurlitzer-2.0.1-py2.py3-none-any.whl\n",
            "Collecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml==3.0.2) (2.6.0)\n",
            "Collecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml==3.0.2) (7.352.0)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml==3.0.2) (2.10)\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=b4795ece5a011d0faed00fc2c825d5a241e54d2ac170c3ec2b6ba97eafcb809f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "Installing collected packages: websocket-client, configobj, everett, wurlitzer, comet-git-pure, netifaces, comet-ml\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.0.2 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 websocket-client-0.57.0 wurlitzer-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vHwQVrRWBgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "3b656b18-e85b-4549-a55b-f99a5d1b1c73"
      },
      "source": [
        "!pip install omegaconf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/95/ebd73361f9c6e94bd0f3b19ffe31c24e833834c022f1c0328ac71b2d6c90/omegaconf-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from omegaconf) (5.3.1)\n",
            "Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf) (3.7.4.2)\n",
            "Installing collected packages: omegaconf\n",
            "Successfully installed omegaconf-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yA6EwQM6P8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "0dfe679a-88c9-4cdc-81d8-c98b24d5e010"
      },
      "source": [
        "!pip install adabound"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.5.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.18.5)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMNgXeWwNx-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "521241cd-9097-4098-da3a-71bc7d2e3c05"
      },
      "source": [
        "!pip install ml_metrics"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ml_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/e7/c31a2dd37045a0c904bee31c2dbed903d4f125a6ce980b91bae0c961abb8/ml_metrics-0.1.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ml_metrics) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ml_metrics) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ml_metrics) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ml_metrics) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->ml_metrics) (1.15.0)\n",
            "Building wheels for collected packages: ml-metrics\n",
            "  Building wheel for ml-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-metrics: filename=ml_metrics-0.1.4-cp36-none-any.whl size=7850 sha256=4c96be29d4d35a67a4b7cfad5648a20405ba81588e3a54aafe24d966ed354a94\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/61/2d/776be7b8a4f14c5db48c8e5451451cabc58dc6aa7ee3801163\n",
            "Successfully built ml-metrics\n",
            "Installing collected packages: ml-metrics\n",
            "Successfully installed ml-metrics-0.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0YvGjijGxET",
        "colab_type": "text"
      },
      "source": [
        "# Import the dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMu2vhSRZD2",
        "colab_type": "text"
      },
      "source": [
        "Import all the libraries neccesary to run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNaJbRoaa8ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from comet_ml import Experiment as CometExperiment\n",
        "from comet_ml import ExistingExperiment as CometExistingExperiment\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from omegaconf.dictconfig import DictConfig\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# from tqdm.notebook import trange, tqdm\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning.logging import LightningLoggerBase\n",
        "from pytorch_lightning.loggers import CometLogger\n",
        "\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "import pickle\n",
        "import adabound\n",
        "import ml_metrics as metrics\n",
        "import random\n",
        "import itertools\n",
        "from torchvision import transforms\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfDyM4E7G4L2",
        "colab_type": "text"
      },
      "source": [
        "# Connect to gDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLY4l7XzReoa",
        "colab_type": "text"
      },
      "source": [
        "Connect the notebook with the gDrive, which is essential to load and save data like dataset, checkpoints, and attention weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQwtSRCbchS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "26c7f9ce-aa16-4b69-aea8-38817cb64731"
      },
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9MDWroJSkhM",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIDIGrXVSzdy",
        "colab_type": "text"
      },
      "source": [
        "This is important to load the k different partitions which are obtained using cross validation k-fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mir9w6zDzkSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/gdrive/My Drive/Proyecto_RecSys/dataset/train_splits.pkl'\n",
        "test_path = '/content/gdrive/My Drive/Proyecto_RecSys/dataset/test_splits.pkl'\n",
        "champion_path = '/content/gdrive/My Drive/Proyecto_RecSys/dataset/champion_types.pkl'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDP51QcmNgCd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Cargar listas de particiones\n",
        "with open(train_path, 'rb') as handle:\n",
        "    list_trainset = pickle.load(handle)\n",
        "\n",
        "with open(test_path, 'rb') as handle:\n",
        "    list_testset = pickle.load(handle)\n",
        "\n",
        "with open(champion_path, 'rb') as handle:\n",
        "    champion_types = pickle.load(handle)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zbdC7M1S3kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_partition(id_split, list_splits = list_trainset):\n",
        "    df = list_splits[id_split]\n",
        "    null_registers = df.loc[(df.item1 == 0) & (df.item2 == 0) & (df.item3 == 0) & (df.item4 == 0) & (df.item5 == 0) & (df.item6 == 0)]\n",
        "    match_to_del = list(set(null_registers['matchid']))\n",
        "    df = df[~df.matchid.isin(match_to_del)]\n",
        "    return df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-OEg1dTYJw",
        "colab_type": "text"
      },
      "source": [
        "These transformations rote randomly the order between the two teams, and the champions within each team."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muCDKh3n524z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomSort_Team(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_random_sample(self, sample):\n",
        "        x, y = sample\n",
        "\n",
        "        ids_teams_1 = [x for x in range(5)]\n",
        "        ids_teams_2 = [x for x in range(5,10)]\n",
        "\n",
        "        ids_team_t = [ids_teams_1, ids_teams_2]\n",
        "\n",
        "        ids_teams = [1, 0]\n",
        "        #ids_teams = [x for x in range(2)]\n",
        "        #random.shuffle(ids_teams)\n",
        "\n",
        "        ids_team_t = [ids_team_t[i] for i in ids_teams]\n",
        "        \n",
        "        ids_team_t = list(itertools.chain.from_iterable(ids_team_t))\n",
        "\n",
        "        x['champions'] = x['champions'][ids_team_t]\n",
        "        x['role'] = x['role'][ids_team_t]\n",
        "        x['type'] = x['type'][ids_team_t,:]\n",
        "\n",
        "        y['items'] = y['items'][ids_team_t,:]\n",
        "\n",
        "        if ids_teams == [1, 0]:\n",
        "            y['win'] = torch.tensor(1) - y['win']\n",
        "        \n",
        "        return x, y\n",
        "\n",
        "    def __call__(self, sample_list):\n",
        "        list_x_champions = []\n",
        "        list_x_role = []\n",
        "        list_x_type = []\n",
        "        list_y_items = []\n",
        "        list_y_win = []\n",
        "        x_old, y_old = sample_list\n",
        "        if isinstance(x_old, (list)) and isinstance(y_old, (list)):\n",
        "            for i in range(len(x_old)):\n",
        "                list_x_champions.append(x_old[i]['champions'])\n",
        "                list_x_role.append(x_old[i]['role'])\n",
        "                list_x_type.append(x_old[i]['type'])\n",
        "                list_y_items.append(y_old[i]['items'])\n",
        "                list_y_win.append(y_old[i]['win'])\n",
        "                sample = x_old[i], y_old[i]\n",
        "                x, y = self.get_random_sample(sample)\n",
        "                list_x_champions.append(x['champions'])\n",
        "                list_x_role.append(x['role'])\n",
        "                list_x_type.append(x['type'])\n",
        "                list_y_items.append(y['items'])\n",
        "                list_y_win.append(y['win'])\n",
        "        else:\n",
        "            list_x_champions.append(x_old['champions'])\n",
        "            list_x_role.append(x_old['role'])\n",
        "            list_x_type.append(x_old['type'])\n",
        "            list_y_items.append(y_old['items'])\n",
        "            list_y_win.append(y_old['win'])\n",
        "            sample = x_old, y_old\n",
        "            x, y = self.get_random_sample(sample)\n",
        "            list_x_champions.append(x['champions'])\n",
        "            list_x_role.append(x['role'])\n",
        "            list_x_type.append(x['type'])\n",
        "            list_y_items.append(y['items'])\n",
        "            list_y_win.append(y['win'])\n",
        "        new_x = {\n",
        "            'champions': torch.stack(list_x_champions, dim=0),\n",
        "            'role': torch.stack(list_x_role, dim=0),\n",
        "            'type': torch.stack(list_x_type, dim=0)\n",
        "        }\n",
        "        new_y = {\n",
        "            'items': torch.stack(list_y_items, dim=0),\n",
        "            'win': torch.stack(list_y_win, dim=0)\n",
        "        }\n",
        "        return new_x, new_y\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozFSmovk06GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomSort_Part(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "        \n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        list_t_x = []\n",
        "        list_t_y = []\n",
        "        x, y = sample\n",
        "\n",
        "        list_t_x.append(x)\n",
        "        list_t_y.append(y)\n",
        "\n",
        "        ids_team_1 = [x for x in range(5)]\n",
        "        ids_team_2 = [x for x in range(5,10)]\n",
        "        random.shuffle(ids_team_1)\n",
        "        random.shuffle(ids_team_2)\n",
        "\n",
        "        ids_match = ids_team_1\n",
        "        ids_match.extend(ids_team_2)\n",
        "        \n",
        "        x['champions'] = x['champions'][ids_match]\n",
        "        x['role'] = x['role'][ids_match]\n",
        "        x['type'] = x['type'][ids_match,:]\n",
        "\n",
        "        y['items'] = y['items'][ids_match,:]\n",
        "\n",
        "        list_t_x.append(x)\n",
        "        list_t_y.append(y)\n",
        "\n",
        "        return list_t_x, list_t_y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3KCpfKDPX3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LolDataset(Dataset):\n",
        "  def __init__(self, data, transform=None):\n",
        "  # cargar el dataset\n",
        "    #self.matches = self._load_matches(path)\n",
        "    self.matches = data\n",
        "    # comprobar si existe el .pkl con los diccionarios\n",
        "\n",
        "    # else:\n",
        "    # extraer info. del dataframe\n",
        "    self.champions = set(self.matches['championid'])\n",
        "    self.roles = set(self.matches['position-role'])\n",
        "    self.matches_id = list(set(self.matches['matchid']))\n",
        "    self.items = self.matches['item1']\n",
        "    self.items.append(self.matches['item2'])\n",
        "    self.items.append(self.matches['item3'])\n",
        "    self.items.append(self.matches['item4'])\n",
        "    self.items.append(self.matches['item5'])\n",
        "    self.items.append(self.matches['item6'])\n",
        "    items = set(self.items)\n",
        "    self.items = {i for i in items if i != 0}\n",
        "    self.champion_types = champion_types\n",
        "    list_champion_types = []\n",
        "    for k,v in champion_types.items():\n",
        "      list_champion_types.extend(v)\n",
        "  \n",
        "    self.set_champ_type = set(list_champion_types)\n",
        "\n",
        "    # crear diccionarios token2id y id2token\n",
        "    self.champions_token2id, self.champions_id2token = self._token_dict(self.champions)\n",
        "    self.roles_token2id, self.roles_id2token = self._token_dict(self.roles)\n",
        "    self.items_token2id, self.items_id2token = self._token_dict(self.items)\n",
        "    self.types_token2id, self.types_id2token = self._token_dict(self.set_champ_type)\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "  def _load_matches(self, path):\n",
        "    data_matches = pd.read_csv(path) \n",
        "    return data_matches\n",
        "\n",
        "  def _token_dict(self, data):\n",
        "    token2id = {}\n",
        "    id2token = {}\n",
        "    for i, j in enumerate(data):\n",
        "      token2id.update({j:i})\n",
        "      id2token.update({i:j})\n",
        "\n",
        "    return token2id, id2token\n",
        "\n",
        "  def _tokens2ids(self, token2id, tokens):\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "      ids.append(token2id[token])\n",
        "      \n",
        "    return ids\n",
        "\n",
        "  def _tokens2ids_items(self, token2id, tokens):\n",
        "    #items_vecs = []\n",
        "    item_vec = np.zeros((len(token2id)))\n",
        "    for token in tokens:\n",
        "      if token in token2id: \n",
        "        item_vec[token2id[token]] = 1\n",
        "      #items_vecs.append(item_vec)\n",
        "      \n",
        "    return item_vec\n",
        "\n",
        "  def _build_dict(self, match):\n",
        "    # sacar en orden los campeones de la partida\n",
        "    champion_tokens = list(match['championid'])\n",
        "    champions_ids = self._tokens2ids(self.champions_token2id, champion_tokens)\n",
        "\n",
        "    # sacar en orden los items de la partida\n",
        "    #items_tokens = match['championid']\n",
        "    #items_ids = self._tokens2ids(self.items_token2id, items_tokens)\n",
        "    # sacar en orden los roles de la partida\n",
        "    role_tokens = list(match['position-role'])\n",
        "    role_ids = self._tokens2ids(self.roles_token2id, role_tokens)\n",
        "    list_win = list(match['win'])[4:6]\n",
        "    \n",
        "    list_win = np.array(list_win)\n",
        "    num_win = np.argsort(list_win)\n",
        "    num_win = num_win[len(num_win)-1]\n",
        "\n",
        "    list_part_items = []\n",
        "    list_types = []\n",
        "    items_list = ['item1','item2','item3','item4','item5','item6']\n",
        "    for id_champ in champion_tokens:\n",
        "      champ_atr = match[match.championid == id_champ]\n",
        "      items = champ_atr[items_list]\n",
        "      items_tokens = list(items.iloc[0, :])\n",
        "      items_ids = self._tokens2ids_items(self.items_token2id, items_tokens)\n",
        "      list_part_items.append(items_ids)\n",
        "\n",
        "      type_champ = self.champion_types[id_champ]\n",
        "      type_ids = self._tokens2ids(self.types_token2id, type_champ)\n",
        "      list_types.append(type_ids)\n",
        "\n",
        "    # construir 5 veces 0s y 5 veces 1s\n",
        "    #team_ids = \n",
        "    x = {\n",
        "        'champions': torch.from_numpy(np.array(champions_ids)),\n",
        "        'role': torch.from_numpy(np.array(role_ids)),\n",
        "        'type': torch.from_numpy(np.array(list_types))\n",
        "    }\n",
        "    y= {\n",
        "        'items': torch.from_numpy(np.array(list_part_items)),\n",
        "        'win': torch.from_numpy(np.array(num_win))\n",
        "    }\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    # idx es el match_id en este caso\n",
        "    # la función debiera retornar la info de cada partida\n",
        "    # buscar idx de la partida en mi estructura, y retornar los diccionarios con los atributos\n",
        "    id_match = self.matches_id[idx]\n",
        "    match = self.matches[(self.matches.matchid == id_match)]\n",
        "    x, y = self._build_dict(match) # entrega un df de la partida según el idx\n",
        "    if self.transform:\n",
        "        sample = x, y\n",
        "        x, y = self.transform(sample)\n",
        "    return x, y # el item per sé, la partida con todas sus características\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.matches_id)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIm1_KUCUNB0",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr3TZbrnUg2H",
        "colab_type": "text"
      },
      "source": [
        "## Transformer encoder modified to obtain the attention weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HQvpd9lxxQqI",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"TransformerEncoder is a stack of N encoder layers\n",
        "\n",
        "    Args:\n",
        "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "        norm: the layer normalization component (optional).\n",
        "\n",
        "    Examples::\n",
        "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        >>> src = torch.rand(10, 32, 512)\n",
        "        >>> out = transformer_encoder(src)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        \"\"\"Pass the input through the endocder layers in turn.\n",
        "\n",
        "        Args:\n",
        "            src: the sequnce to the encoder (required).\n",
        "            mask: the mask for the src sequence (optional).\n",
        "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "\n",
        "        Shape:\n",
        "            see the docs in Transformer class.\n",
        "        \"\"\"\n",
        "        output = src\n",
        "        att_weights = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            output, attn_output_weights = self.layers[i](output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "            \n",
        "            att_weights.append(attn_output_weights)\n",
        "\n",
        "        if self.norm:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, att_weights\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh2Nz2CWDypX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    else:\n",
        "        raise RuntimeError(\"activation should be relu/gelu, not %s.\" % activation)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJCX_mqojgwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
        "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
        "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
        "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
        "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
        "    in a different way during application.\n",
        "\n",
        "    Args:\n",
        "        d_model: the number of expected features in the input (required).\n",
        "        nhead: the number of heads in the multiheadattention models (required).\n",
        "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "\n",
        "    Examples::\n",
        "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        >>> src = torch.rand(10, 32, 512)\n",
        "        >>> out = encoder_layer(src)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        \"\"\"Pass the input through the endocder layer.\n",
        "\n",
        "        Args:\n",
        "            src: the sequnce to the encoder layer (required).\n",
        "            src_mask: the mask for the src sequence (optional).\n",
        "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "\n",
        "        Shape:\n",
        "            see the docs in Transformer class.\n",
        "        \"\"\"\n",
        "        src2, attn_output_weights = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        if hasattr(self, \"activation\"):\n",
        "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        else:  # for backward compatibility\n",
        "            src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src, attn_output_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRy106QU6sH",
        "colab_type": "text"
      },
      "source": [
        "## Auxiliary Task Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNyvw9ynsLq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getItems(gt_items, table_emb, num_items, emb_dim):\n",
        "    list_match = []\n",
        "    device = gt_items.device\n",
        "    for i in range(gt_items.size(0)):\n",
        "        match = gt_items[i,:,:]\n",
        "        list_part_item = []\n",
        "        for j in range(gt_items.size(1)):\n",
        "            participant_items = match[j,:]\n",
        "            sum_k = torch.sum(participant_items, dim = 0).item()\n",
        "            if int(sum_k) > 0:\n",
        "                _, pos_items = torch.topk(participant_items, k = int(sum_k), dim = 0)\n",
        "                items_emb = table_emb(pos_items)\n",
        "                items_emb = torch.mean(items_emb, dim = 0)\n",
        "                list_part_item.append(items_emb)\n",
        "            else:\n",
        "                list_part_item.append(torch.zeros(emb_dim).to(device))\n",
        "        team_item_emb = torch.stack(list_part_item)\n",
        "        list_match.append(team_item_emb)\n",
        "    return torch.stack(list_match)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KahjODMTdPS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WinEncoder(nn.Module):\n",
        "    def __init__(self, model_dim, n_items):\n",
        "        super(WinEncoder, self).__init__()\n",
        "        self.proj_win = nn.Linear(4*model_dim, 2)\n",
        "        self.embeddings_table_items = nn.Embedding(num_embeddings = n_items, embedding_dim = model_dim)\n",
        "        self.n_items = n_items\n",
        "        self.model_dim = model_dim\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.proj_win.bias.data.zero_()\n",
        "        self.proj_win.weight.data.uniform_(-initrange, initrange)\n",
        "        self.embeddings_table_items.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, att_match, item_list):\n",
        "        # att_match size (Batch, Seq, Emb)\n",
        "        # item_list size (Batch, Seq, Num_items, Emb)\n",
        "        att_item_team_1, att_item_team_2 = torch.chunk(att_match, 2, dim=1)\n",
        "        items_team_1, items_team_2 = torch.chunk(item_list, 2, dim=1)\n",
        "\n",
        "        items_team_1 = getItems(items_team_1, self.embeddings_table_items, self.n_items,self.model_dim)\n",
        "        items_team_2 = getItems(items_team_2, self.embeddings_table_items, self.n_items,self.model_dim)\n",
        "\n",
        "        att_item_team_1 = torch.mean(att_item_team_1, dim=1)\n",
        "        att_item_team_1 = F.relu(att_item_team_1)\n",
        "        att_item_team_1 = (att_item_team_1 / att_item_team_1.max())\n",
        "        items_team_1 = torch.mean(items_team_1, dim=1)\n",
        "        items_team_1 = F.relu(items_team_1)\n",
        "        items_team_1 = (items_team_1 / items_team_1.max())\n",
        "\n",
        "        att_item_team_2 = torch.mean(att_item_team_2, dim=1)\n",
        "        att_item_team_2 = F.relu(att_item_team_2)\n",
        "        att_item_team_2 = (att_item_team_2 / att_item_team_2.max())\n",
        "        items_team_2 = torch.mean(items_team_2, dim=1)\n",
        "        items_team_2 = F.relu(items_team_2)\n",
        "        items_team_2 = (items_team_2 / items_team_2.max())\n",
        "\n",
        "        att_item_team_1 = torch.cat((att_item_team_1, items_team_1), 1)\n",
        "        att_item_team_2 = torch.cat((att_item_team_2, items_team_2), 1)\n",
        "        proj_win_team = torch.cat((att_item_team_1, att_item_team_2), 1)\n",
        "        win_emb = self.proj_win(F.relu(proj_win_team))\n",
        "\n",
        "        return win_emb"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzByFr9ZyB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTensorPredItem(items_logits):\n",
        "  pred_items = torch.zeros(items_logits.size())\n",
        "  for i in range(items_logits.size(0)):\n",
        "    for j in range(items_logits.size(1)):\n",
        "      _,pos_items = torch.topk(items_logits[i,j,:],k = 6,dim=0)\n",
        "      pred_items[i,j,pos_items] = 1\n",
        "  return pred_items"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlU_u42VG8A",
        "colab_type": "text"
      },
      "source": [
        "## Main Class of the proposed model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY-TRDX2BHAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerLolRecommender(nn.Module):\n",
        "\n",
        "    def __init__(self, n_role, n_champions, embeddings_size, nhead, n_items, n_type, nlayers = 1, nhid = 2048, dropout=0.5, aux_task = False, \n",
        "                 learnable_team_emb = False):\n",
        "        super(TransformerLolRecommender, self).__init__()\n",
        "\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        \n",
        "        self.embeddings_table_role = nn.Embedding(num_embeddings = n_role, embedding_dim = embeddings_size)\n",
        "        \n",
        "        self.embeddings_table_champion = nn.Embedding(num_embeddings = n_champions, embedding_dim = embeddings_size)\n",
        "\n",
        "        self.embeddings_table_type = nn.Embedding(num_embeddings = n_type, embedding_dim = embeddings_size, padding_idx=0)\n",
        "        \n",
        "        self.learnable_team_emb = learnable_team_emb\n",
        "        if learnable_team_emb:\n",
        "            self.team_encoder = nn.Embedding(num_embeddings = 2, embedding_dim = embeddings_size)\n",
        "        else:\n",
        "            self.team_encoder = self.get_team_encoding(embeddings_size, 10)\n",
        "        \n",
        "        encoder_layers = TransformerEncoderLayer(embeddings_size, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        \n",
        "        self.recommender = nn.Linear(embeddings_size, n_items)\n",
        "        self.pred_champ = nn.Linear(embeddings_size, n_champions)\n",
        "\n",
        "        self.aux_task = aux_task\n",
        "\n",
        "        if self.aux_task: \n",
        "            self.win_encoder = WinEncoder(embeddings_size, n_items)\n",
        "\n",
        "        self.init_weights()\n",
        "    \n",
        "    def get_learnable_team_emb(self, num_batch):\n",
        "        emb_team_0 = self.team_encoder(torch.LongTensor([0]).to(self.device))\n",
        "        emb_team_0 = emb_team_0.expand(5, emb_team_0.size(1))\n",
        "        emb_team_1 = self.team_encoder(torch.LongTensor([1]).to(self.device))\n",
        "        emb_team_1 = emb_team_1.expand(5, emb_team_1.size(1))\n",
        "        emb_team = torch.cat([emb_team_0, emb_team_1], dim = 0)\n",
        "        emb_team = emb_team.unsqueeze(0).expand(num_batch, emb_team.size(0), emb_team.size(1))\n",
        "        return emb_team\n",
        "\n",
        "    \n",
        "    def get_team_encoding(self, embedding_dim, num_champions = 10):\n",
        "        team_encoding = torch.zeros(num_champions, embedding_dim)\n",
        "        team_encoding[5:,:] = 1\n",
        "        return team_encoding.to(self.device)\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        \n",
        "        self.embeddings_table_role.weight.data.uniform_(-initrange, initrange)\n",
        "        self.embeddings_table_champion.weight.data.uniform_(-initrange, initrange)\n",
        "        self.embeddings_table_type.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "        self.recommender.bias.data.zero_()\n",
        "        self.recommender.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        self.pred_champ.bias.data.zero_()\n",
        "        self.pred_champ.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        if self.learnable_team_emb:\n",
        "            self.team_encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, role, champion_id, types, items, win, enable_teacher_f):\n",
        "\n",
        "        role_participants = self.embeddings_table_role(role)\n",
        "        id_participants = self.embeddings_table_champion(champion_id)\n",
        "        type_champ = self.embeddings_table_type(types)\n",
        "        type_champ = torch.sum(type_champ, dim =2)\n",
        "        batch_size = role_participants.size(0)\n",
        "        if self.learnable_team_emb:\n",
        "            team_participants = self.get_learnable_team_emb(batch_size)\n",
        "        else:\n",
        "            size_team_emb = self.team_encoder.size()\n",
        "            team_participants = self.team_encoder.unsqueeze(0).expand(batch_size, size_team_emb[0], size_team_emb[1])\n",
        "\n",
        "        sel_champions = []\n",
        "        pos_champions = []\n",
        "        for i in range(win.size(0)):\n",
        "            id_el = random.randint(0,4)\n",
        "            pos_champions.append(id_el)\n",
        "            if win[i] != 0:\n",
        "                id_el = id_el + 5\n",
        "            sel_champion = champion_id[i,id_el]\n",
        "            id_participants[i,id_el,:] = 0\n",
        "            sel_champions.append(sel_champion)\n",
        "\n",
        "        sel_champions = torch.stack(sel_champions)\n",
        "        # pos_champions = torch.stack(pos_champions)\n",
        "\n",
        "        participants = role_participants + id_participants + team_participants + type_champ\n",
        "        # size (Seq, Batch, Emb)\n",
        "        participants = participants.permute(1,0,2)\n",
        "        # size (Seq, Batch, Emb)\n",
        "        output, att_weights = self.transformer_encoder(participants)\n",
        "        # size (Batch, Seq, Emb)\n",
        "        output = output.permute(1,0,2)\n",
        "        logits_items = self.recommender(output)\n",
        "\n",
        "        output_obj = {\n",
        "            'logits_items': logits_items,\n",
        "            'att_weights': att_weights,\n",
        "            'outputs': output,\n",
        "            'sel_champions': sel_champions,\n",
        "            'pos_champions': pos_champions\n",
        "        }\n",
        "\n",
        "        if self.aux_task:\n",
        "            if enable_teacher_f: \n",
        "                items_used = items\n",
        "            else:\n",
        "                items_used = getTensorPredItem(logits_items).to(self.device)\n",
        "            logits_win = self.win_encoder(output, items_used)\n",
        "            output_obj['logits_win'] = logits_win\n",
        "\n",
        "        return output_obj"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwYoKWcsVqex",
        "colab_type": "text"
      },
      "source": [
        "# Logger and Checkpointer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrXJc5reVwyM",
        "colab_type": "text"
      },
      "source": [
        "These classes and methods are essential to log relevant information about the model and metrics in Coment. Likewise, they allow to save checkpoint in each epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0JLqdBEV67X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_defaults(defaults_file):\n",
        "    return OmegaConf.load(defaults_file)\n",
        "\n",
        "\n",
        "def load_config_file(config_file):\n",
        "    if not config_file:\n",
        "        return OmegaConf.create()\n",
        "    return OmegaConf.load(config_file)\n",
        "\n",
        "\n",
        "def load_config(config_file, defaults_file):\n",
        "    defaults = load_defaults(defaults_file)\n",
        "    config = OmegaConf.merge(defaults, load_config_file(config_file))\n",
        "    config.merge_with_cli()\n",
        "    return config\n",
        "\n",
        "\n",
        "def build_config(args):\n",
        "    return load_config(args.config_file, args.defaults_file)\n",
        "\n",
        "\n",
        "def config_to_dict(cfg):\n",
        "    return dict(cfg)\n",
        "\n",
        "\n",
        "def config_to_comet(cfg):\n",
        "    def _config_to_comet(cfg, local_dict, parent_str):\n",
        "        for key, value in cfg.items():\n",
        "            full_key = \"{}.{}\".format(parent_str, key)\n",
        "            if isinstance(value, (dict, DictConfig)):\n",
        "                _config_to_comet(value, local_dict, full_key)\n",
        "            else:\n",
        "                local_dict[full_key] = value\n",
        "\n",
        "    local_dict = {}\n",
        "    for key, value in cfg.items():\n",
        "        if isinstance(value, (dict, DictConfig)):\n",
        "            _config_to_comet(value, local_dict, key)\n",
        "        else:\n",
        "            local_dict[key] = value\n",
        "    return local_dict"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWTpFcpaU50q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_checkpointer(save_path, metric_name='val_acc'):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    return ModelCheckpoint(\n",
        "        filepath=save_path,\n",
        "        verbose=True,\n",
        "        monitor=metric_name,\n",
        "        mode='max',\n",
        "    )\n",
        "\n",
        "\n",
        "# class CometLogger(LightningLoggerBase):\n",
        "#     # Thank you @ceyzaguirre4\n",
        "#     def __init__(self, config, *args, **kwargs):\n",
        "#         super().__init__()\n",
        "#         self.comet_exp = CometExperiment(*args, **kwargs)\n",
        "#         self.comet_exp.set_name(config['exp_name'])\n",
        "#         self.comet_exp.log_parameters(config)\n",
        "#         self.config = config\n",
        "\n",
        "#     @rank_zero_only\n",
        "#     def log_hyperparams(self, params):\n",
        "#         self.comet_exp.log_parameters(config_to_comet(params))\n",
        "\n",
        "#     @rank_zero_only\n",
        "#     def log_metrics(self, metrics, step):\n",
        "#         self.comet_exp.log_metrics(metrics)\n",
        "\n",
        "#     @rank_zero_only\n",
        "#     def finalize(self, status):\n",
        "#         self.comet_exp.end()\n",
        "    \n",
        "#     def version(self):\n",
        "#         return self.config['exp']\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ktMqAUMWeEz",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMnKThtdtb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall_at_k(output, target, k = 6):\n",
        "    output_k, ind_k = torch.topk(output, k, dim = 1)\n",
        "    sum_recall = 0\n",
        "    num_part = output_k.size(0)\n",
        "    relevants = target.sum(dim = 1)\n",
        "    list_recall = []\n",
        "    for i in range(num_part):\n",
        "      target_k = target[i, ind_k[i,:]]\n",
        "      intersection = target_k.sum(dim = 0)\n",
        "      recall_n = intersection/relevants[i]\n",
        "      list_recall.append(recall_n)\n",
        "      sum_recall+=recall_n\n",
        "    \n",
        "    recall_avg = sum_recall/num_part\n",
        "    return recall_avg, num_part, list_recall\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xRNZaAJ_RZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_at_k(r, k):\n",
        "    \"\"\"Score is precision @ k\n",
        "\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    >>> r = [0, 0, 1]\n",
        "    >>> precision_at_k(r, 1)\n",
        "    0.0\n",
        "    >>> precision_at_k(r, 2)\n",
        "    0.0\n",
        "    >>> precision_at_k(r, 3)\n",
        "    0.33333333333333331\n",
        "    >>> precision_at_k(r, 4)\n",
        "    Traceback (most recent call last):\n",
        "        File \"<stdin>\", line 1, in ?\n",
        "    ValueError: Relevance score length < k\n",
        "\n",
        "\n",
        "    Args:\n",
        "        r: Relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Precision @ k\n",
        "\n",
        "    Raises:\n",
        "        ValueError: len(r) must be >= k\n",
        "    \"\"\"\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k] != 0\n",
        "    if r.size != k:\n",
        "        raise ValueError('Relevance score length < k')\n",
        "    return np.mean(r)\n",
        "\n",
        "\n",
        "def average_precision(r):\n",
        "    \"\"\"Score is average precision (area under PR curve)\n",
        "\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
        "    >>> delta_r = 1. / sum(r)\n",
        "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
        "    0.7833333333333333\n",
        "    >>> average_precision(r)\n",
        "    0.78333333333333333\n",
        "\n",
        "    Args:\n",
        "        r: Relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Average precision\n",
        "    \"\"\"\n",
        "    r = np.asarray(r) != 0\n",
        "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.mean(out)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfusZ9qS_uSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_at(output, target, k=6):\n",
        "    sum_ap = 0\n",
        "    num_part = output.size(0)\n",
        "    list_map = []\n",
        "    for i in range(num_part):\n",
        "      out_p = output[i,:]\n",
        "      target_p = target[i,:]\n",
        "      output_k, ind_k = torch.topk(out_p, k, dim = 0)\n",
        "      list_rel = target_p[ind_k].tolist()\n",
        "      ap_at = average_precision(list_rel)\n",
        "      list_map.append(ap_at) \n",
        "      sum_ap += ap_at\n",
        "    return sum_ap/num_part, list_map"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CctZXsremks4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_precision_multiclass(output, target, k = 6):\n",
        "    output_k, ind_k = torch.topk(output, k, dim = 1)\n",
        "    sum_prec = 0\n",
        "    num_part = output_k.size(0)\n",
        "    list_prec = []\n",
        "    for i in range(num_part):\n",
        "      target_k = target[i, ind_k[i,:]]\n",
        "      intersection = target_k.sum(dim = 0)\n",
        "      preci_n = intersection/k\n",
        "      list_prec.append(preci_n)\n",
        "      sum_prec+=preci_n\n",
        "    \n",
        "    prec_avg = sum_prec/num_part\n",
        "    return prec_avg, num_part, list_prec"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykasAMS0bnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1_score(recall, precision):\n",
        "  f1 = 2 * ((precision * recall) / (precision + recall))\n",
        "  return f1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zMdgUJslL4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\n",
        "    Taken from PyTorch's examples.imagenet.main\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x01LspaO_A7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(seed, slow=False):\n",
        "    import random\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if slow:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2URNBE4I7x7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_winners(att_vec, gt_item, win_vec, pos_champions, outputs_log):\n",
        "    list_att = []\n",
        "    list_gt = []\n",
        "    list_cham = []\n",
        "    for i in range(att_vec.size(0)):\n",
        "        win = win_vec[i]\n",
        "        pos = pos_champions[i]\n",
        "        if win == 0:\n",
        "            a = list(range(0,5))\n",
        "            del a[pos]\n",
        "            att_vec_match = att_vec[i,a,:]\n",
        "            gt_match = gt_item[i,a, :]\n",
        "            list_cham.append(outputs_log[i,pos, :])\n",
        "            list_att.append(att_vec_match)\n",
        "            list_gt.append(gt_match)          \n",
        "        else:\n",
        "            a = list(range(5,10))\n",
        "            del a[pos]\n",
        "            att_vec_match = att_vec[i,a,:]\n",
        "            gt_match = gt_item[i, a, :]\n",
        "            list_cham.append(outputs_log[i,pos + 5, :])\n",
        "            list_att.append(att_vec_match)\n",
        "            list_gt.append(gt_match)\n",
        "\n",
        "    att_winners = torch.stack(list_att, dim=0)\n",
        "    gt_winners = torch.stack(list_gt, dim=0)\n",
        "    att_cham = torch.stack(list_cham, dim=0)\n",
        "    return att_winners, gt_winners, att_cham\n",
        "    \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3xn5Zd30i3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_att_weights(list_att, path_save_att):\n",
        "  with open(path_save_att, 'wb') as handle:\n",
        "    pickle.dump(list_att, handle)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDA0GHysW4vX",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVeG7SOJW80S",
        "colab_type": "text"
      },
      "source": [
        "The training and evaluation loop are based on [Pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOJROUqzSqJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR77pKXdXDDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Struct:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "        #self.elems = entries.items()\n",
        "    \n",
        "    def items(self):\n",
        "        return self.__dict__.items()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxtiaNMYc6iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LolRecAttModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super(LolRecAttModel, self).__init__()\n",
        "        \n",
        "        if type(cfg) is argparse.Namespace:\n",
        "          cfg = vars(cfg)\n",
        "        self.conf = cfg\n",
        "        self.hparams = cfg\n",
        "        self.index_split = self.conf['index_split']\n",
        "        self.optim = self.conf['optim']\n",
        "        set_seed(seed = self.conf['seed'])\n",
        "        train_dataset = self.train_dataset()\n",
        "        self.batch_size = self.conf['batch_size']\n",
        "        self.iter_max_train = len(train_dataset)//self.batch_size\n",
        "        num_roles = len(train_dataset.roles)\n",
        "        num_champions = len(train_dataset.champions)\n",
        "        n_items = len(train_dataset.items)\n",
        "        n_types = len(train_dataset.set_champ_type)\n",
        "        self.model = TransformerLolRecommender(n_role=num_roles, n_champions=num_champions, embeddings_size=self.conf['embeddings_size'], nhead=self.conf['nhead'], n_items=n_items, n_type=n_types,\n",
        "                                               nlayers = self.conf['nlayers'], nhid = self.conf['nhid'], dropout=self.conf['dropout'], aux_task = self.conf['win_task'], \n",
        "                                               learnable_team_emb = self.conf['learnable_team_emb'])\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        self.loss_aux = nn.CrossEntropyLoss()\n",
        "        self.train_loss = AverageMeter()\n",
        "        self.train_prec = AverageMeter()\n",
        "        self.iter_epoch = 0\n",
        "        isExist = os.path.exists(path_save) \n",
        "        if isExist:\n",
        "          dirs = os.listdir(path_save)\n",
        "          self.iter_epoch = len(dirs)\n",
        "\n",
        "        self.aux_task = self.conf['win_task']\n",
        "        \n",
        "        if self.aux_task:\n",
        "            self.second_loss = nn.CrossEntropyLoss()\n",
        "            self.train_acc_win = AverageMeter()\n",
        "            self.train_main_loss = AverageMeter()\n",
        "            self.train_win_loss = AverageMeter()\n",
        "            self.alpha = self.conf['alpha']\n",
        "            self.beta = self.conf['beta']\n",
        "            self.epoch_to_win = self.conf['init_epoch']\n",
        "\n",
        "    def check_epoch(self, num_iter):\n",
        "      if num_iter == 0:\n",
        "        self.train_loss = AverageMeter()\n",
        "        self.train_prec = AverageMeter()\n",
        "        if self.aux_task:\n",
        "            self.train_acc_win = AverageMeter()\n",
        "            self.train_main_loss = AverageMeter()\n",
        "            self.train_win_loss = AverageMeter()\n",
        "        self.iter_epoch+=1\n",
        "\n",
        "    def custom_print(self, batch,  loss, start_time, prec, acc = 0, log_interval = 100, loss_win =0, epoch=1):\n",
        "      if batch % log_interval == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            elapsed = elapsed*log_interval if batch > 0 else elapsed\n",
        "            if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                      'ms/batch {:5.2f} | '\n",
        "                      'loss {:5.6f} | loss win {:5.6f} | precision {:5.6f} | Accuracy (win) {:5.6f}'.format(\n",
        "                        self.iter_epoch, batch, self.iter_max_train,\n",
        "                        elapsed, loss, loss_win, prec, acc))\n",
        "            else:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                      'ms/batch {:5.2f} | '\n",
        "                      'loss {:5.6f} | precision {:5.6f}'.format(\n",
        "                        self.iter_epoch, batch, self.iter_max_train,\n",
        "                        elapsed, loss, prec)) \n",
        "\n",
        "    def forward(self, x, items, win, teacher_forcing):\n",
        "        role = x['role']\n",
        "        champions = x['champions']\n",
        "        types = x['type']\n",
        "        out = self.model(role, champions, types, items, win, teacher_forcing)\n",
        "        return out\n",
        "        #return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        # REQUIRED\n",
        "        self.check_epoch(batch_nb)\n",
        "        start_time = time.time()\n",
        "        x, y = batch\n",
        "        if len(x['role'].size()) == 3:\n",
        "          x['role'] = x['role'].reshape(x['role'].size(0)*x['role'].size(1), x['role'].size(2))\n",
        "          x['champions'] = x['champions'].reshape(x['champions'].size(0)*x['champions'].size(1), x['champions'].size(2))\n",
        "          x['type'] = x['type'].reshape(x['type'].size(0)*x['type'].size(1), x['type'].size(2), x['type'].size(3))\n",
        "          y['items'] = y['items'].reshape(y['items'].size(0)*y['items'].size(1), y['items'].size(2), y['items'].size(3))\n",
        "          y['win'] = y['win'].reshape(y['win'].size(0)*y['win'].size(1))\n",
        "        y_hat = self.forward(x, y['items'], y['win'], self.conf['teacher_forcing'])\n",
        "        \n",
        "        #Mains task\n",
        "        logits_items = y_hat['logits_items']\n",
        "        gt_items = y['items']\n",
        "        sel_champions = y_hat['sel_champions']\n",
        "        pos_champions = y_hat['pos_champions']\n",
        "        outputs_log = y_hat['outputs']\n",
        "        logits_items, gt_items, att_cham = get_winners(logits_items, gt_items, y['win'], pos_champions, outputs_log)\n",
        "        \n",
        "        out = logits_items.reshape(logits_items.size(0)*logits_items.size(1), logits_items.size(2))\n",
        "        out_aux = self.model.pred_champ(att_cham)\n",
        "\n",
        "        gt = gt_items.reshape(gt_items.size(0)*gt_items.size(1), gt_items.size(2))\n",
        "        loss = self.loss(out, gt)\n",
        "        loss_aux = self.loss_aux(out_aux, sel_champions)\n",
        "\n",
        "        prec, num, _ = calc_precision_multiclass(out, gt, k=6)\n",
        "        self.train_prec.update(prec, num)\n",
        "\n",
        "        tensor_avg_prec = torch.tensor([self.train_prec.avg], device=loss.device)\n",
        "        tensorboard_logs = {'train_loss': loss, 'train_loss_aux': loss_aux, 'train_prec_avg': tensor_avg_prec}\n",
        "\n",
        "        if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "\n",
        "            #Second Task\n",
        "            out_win = y_hat['logits_win']\n",
        "            \n",
        "            gt_win = y['win'].reshape(-1)\n",
        "\n",
        "            _, preds_win = torch.max(out_win, 1)\n",
        "\n",
        "            loss_win = self.second_loss(out_win, gt_win)\n",
        "            loss_total = self.alpha*loss + self.beta*loss_win\n",
        "            self.train_loss.update(self.alpha*loss.item(), out.size(0))\n",
        "            self.train_loss.update(self.beta*loss_win.item(), out_win.size(0))\n",
        "\n",
        "            train_acc = torch.sum(preds_win == gt_win).item()/out_win.size(0)\n",
        "            self.train_acc_win.update(train_acc, out_win.size(0))\n",
        "            self.train_main_loss.update(loss.item(), out.size(0))\n",
        "            self.train_win_loss.update(loss_win.item(), out_win.size(0))\n",
        "\n",
        "            tensor_avg_acc = torch.tensor([self.train_acc_win.avg], device=loss.device)\n",
        "            tensorboard_logs['train_acc_win_avg'] = tensor_avg_acc\n",
        "            tensorboard_logs['train_win_loss'] = loss_win\n",
        "            tensorboard_logs['train_main_loss'] = loss\n",
        "            tensorboard_logs['train_win_loss_avg'] = torch.tensor([self.train_win_loss.avg], device=loss.device)\n",
        "            tensorboard_logs['train_main_loss_avg'] = torch.tensor([self.train_main_loss.avg], device=loss.device)\n",
        "            tensorboard_logs['train_loss'] = loss_total\n",
        "\n",
        "            # self.custom_print(batch_nb, self.train_main_loss.avg, start_time, self.train_prec.avg, self.train_acc_win.avg, 100, self.train_win_loss.avg)\n",
        "        else:\n",
        "            loss_total = loss + 0.2*loss_aux\n",
        "            self.train_loss.update(loss.item(), out.size(0))\n",
        "            tensorboard_logs['total_loss_train'] = loss_total\n",
        "            # self.custom_print(batch_nb, self.train_loss.avg, start_time, self.train_prec.avg, 0, 100)\n",
        "        \n",
        "        tensor_avg_loss = torch.tensor([self.train_loss.avg], device=loss.device)\n",
        "        tensorboard_logs['train_loss_avg'] = tensor_avg_loss\n",
        "        return {'loss': loss_total, 'progress_bar': tensorboard_logs, 'avg_loss':  tensor_avg_loss, 'avg_prec':tensor_avg_prec ,'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        y_hat = self.forward(x, y['items'], y['win'], False)\n",
        "        att_weights = y_hat['att_weights']\n",
        "\n",
        "        #Main Task\n",
        "        logits_items = y_hat['logits_items']\n",
        "        gt_items = y['items']\n",
        "        sel_champions = y_hat['sel_champions']\n",
        "        pos_champions = y_hat['pos_champions']\n",
        "        outputs_log = y_hat['outputs']\n",
        "\n",
        "        logits_items, gt_items, att_cham = get_winners(logits_items, gt_items, y['win'], pos_champions, outputs_log)\n",
        "        out = logits_items.reshape(logits_items.size(0)*logits_items.size(1), logits_items.size(2))\n",
        "        out_aux = self.model.pred_champ(att_cham)\n",
        "      \n",
        "        gt = gt_items.reshape(gt_items.size(0)*gt_items.size(1), gt_items.size(2))\n",
        "\n",
        "        loss = self.loss(out, gt)\n",
        "        loss_aux = self.loss_aux(out_aux, sel_champions)\n",
        "\n",
        "        prec, num, list_prec = calc_precision_multiclass(out, gt, k=6)\n",
        "        prec1, num, list_prec1 = calc_precision_multiclass(out, gt, k=1)\n",
        "        prec3, num, list_prec3 = calc_precision_multiclass(out, gt, k=3)\n",
        "\n",
        "        recall1, num, list_recall1 = recall_at_k(out, gt, k=1)\n",
        "        recall3, num, list_recall3 = recall_at_k(out, gt, k=3)\n",
        "        recall6, num, list_recall6 = recall_at_k(out, gt, k=6)\n",
        "\n",
        "        f11 = f1_score(recall1, prec1)\n",
        "        f13 = f1_score(recall3, prec3) \n",
        "        f16 = f1_score(recall6, prec)\n",
        "\n",
        "        map6, list_map6 = map_at(out, gt, k=6)\n",
        "        map1, list_map1 = map_at(out, gt, k=1)\n",
        "        map3, list_map3 = map_at(out, gt, k=3)\n",
        "\n",
        "        obj_list = {\n",
        "            'list_prec1': list_prec1,\n",
        "            'list_prec3': list_prec3,\n",
        "            'list_prec': list_prec,\n",
        "            'list_recall1': list_recall1,\n",
        "            'list_recall3': list_recall3,\n",
        "            'list_recall6': list_recall6,\n",
        "            'list_map1': list_map1,\n",
        "            'list_map3': list_map3,\n",
        "            'list_map6': list_map6\n",
        "        }\n",
        "        obj_res = {'val_loss': loss, 'val_loss_aux': loss_aux, 'val_prec': prec, 'num_batch': out.size(0), 'num':num, 'map6': map6, \n",
        "                   'map1': map1, 'map3': map3, 'val_prec1': prec1, 'val_prec3': prec3, 'val_recall1': recall1, \n",
        "                   'val_recall3': recall3, 'val_recall6': recall6, 'val_f1_1': f11, 'val_f1_3': f13, 'val_f1_6': f16, \n",
        "                   'att_weights': att_weights, 'logits_items': logits_items, 'obj_list': obj_list}\n",
        "\n",
        "        #Second Task\n",
        "        if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "            out_win = y_hat['logits_win']\n",
        "            \n",
        "            gt_win = y['win'].reshape(-1)\n",
        "            _, preds_win = torch.max(out_win, 1)\n",
        "\n",
        "            loss_win = self.second_loss(out_win, gt_win)\n",
        "\n",
        "            acc_win = torch.sum(preds_win == gt_win).item()/out_win.size(0)\n",
        "            obj_res['val_acc'] = acc_win\n",
        "            obj_res['val_loss_win'] = loss_win\n",
        "            obj_res['val_main_loss'] = loss\n",
        "            obj_res['num_batch_acc'] = out_win.size(0)\n",
        "\n",
        "        return obj_res\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = AverageMeter()\n",
        "        avg_loss_aux = AverageMeter()\n",
        "        avg_prec = AverageMeter()\n",
        "        avg_prec1 = AverageMeter()\n",
        "        avg_prec3 = AverageMeter()\n",
        "\n",
        "        avg_recall1 = AverageMeter()\n",
        "        avg_recall3 = AverageMeter()\n",
        "        avg_recall6 = AverageMeter()\n",
        "\n",
        "        avg_f1_1 = AverageMeter()\n",
        "        avg_f1_3 = AverageMeter()\n",
        "        avg_f1_6 = AverageMeter()\n",
        "\n",
        "        avg_map = AverageMeter()\n",
        "        avg_map1 = AverageMeter()\n",
        "        avg_map3 = AverageMeter()\n",
        "\n",
        "        list_att_weights = []\n",
        "        list_logits_items = []\n",
        "\n",
        "        list_prec1 = []\n",
        "        list_prec3 = []\n",
        "        list_prec6 = []\n",
        "\n",
        "        list_recall1 = []\n",
        "        list_recall3 = []\n",
        "        list_recall6 = []\n",
        "\n",
        "        list_map1 = []\n",
        "        list_map3 = []\n",
        "        list_map6 = []\n",
        "\n",
        "        if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "          avg_main_loss = AverageMeter()\n",
        "          avg_win_loss = AverageMeter()\n",
        "          avg_acc = AverageMeter()\n",
        "\n",
        "        device = None\n",
        "        for x in outputs:\n",
        "\n",
        "          avg_prec.update(x['val_prec'], x['num'])\n",
        "          avg_prec1.update(x['val_prec1'], x['num'])\n",
        "          avg_prec3.update(x['val_prec3'], x['num'])\n",
        "\n",
        "          avg_recall1.update(x['val_recall1'], x['num'])\n",
        "          avg_recall3.update(x['val_recall3'], x['num'])\n",
        "          avg_recall6.update(x['val_recall6'], x['num'])\n",
        "\n",
        "          avg_f1_1.update(x['val_f1_1'], x['num'])\n",
        "          avg_f1_3.update(x['val_f1_3'], x['num'])\n",
        "          avg_f1_6.update(x['val_f1_6'], x['num'])\n",
        "\n",
        "          avg_map.update(x['map6'], x['num_batch'])\n",
        "          avg_map1.update(x['map1'], x['num_batch'])\n",
        "          avg_map3.update(x['map3'], x['num_batch'])\n",
        "\n",
        "          list_att_weights.append(x['att_weights'])\n",
        "          list_logits_items.append(x['logits_items'])\n",
        "\n",
        "          list_prec1.extend(x['obj_list']['list_prec1'])\n",
        "          list_prec3.extend(x['obj_list']['list_prec3'])\n",
        "          list_prec6.extend(x['obj_list']['list_prec'])\n",
        "\n",
        "          list_recall1.extend(x['obj_list']['list_recall1'])\n",
        "          list_recall3.extend(x['obj_list']['list_recall3'])\n",
        "          list_recall6.extend(x['obj_list']['list_recall6'])\n",
        "\n",
        "          list_map1.extend(x['obj_list']['list_map1'])\n",
        "          list_map3.extend(x['obj_list']['list_map3'])\n",
        "          list_map6.extend(x['obj_list']['list_map6'])\n",
        "\n",
        "          device = x['val_loss'].device\n",
        "\n",
        "          if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "            avg_main_loss.update(x['val_main_loss'], x['num_batch'])\n",
        "            avg_win_loss.update(x['val_loss_win'], x['num_batch_acc'])\n",
        "            avg_acc.update(x['val_acc'], x['num_batch_acc'])\n",
        "\n",
        "            avg_loss.update(self.alpha*x['val_main_loss'], x['num_batch'])\n",
        "            avg_loss.update(self.beta*x['val_loss_win'], x['num_batch_acc'])\n",
        "          else:\n",
        "            avg_loss.update(x['val_loss'], x['num_batch'])\n",
        "            avg_loss_aux.update(x['val_loss_aux'], x['num_batch'])\n",
        "\n",
        "        tensorboard_logs = {'val_loss': torch.tensor([avg_loss.avg], device=device), 'val_prec': torch.tensor([avg_prec.avg], device=device), \n",
        "                            'val_map6': torch.tensor([avg_map.avg], device=device), 'val_map1': torch.tensor([avg_map1.avg], device=device),\n",
        "                            'val_map3': torch.tensor([avg_map3.avg], device=device), 'val_prec1': torch.tensor([avg_prec1.avg], device=device), \n",
        "                            'val_prec3': torch.tensor([avg_prec3.avg], device=device), 'val_recall1': torch.tensor([avg_recall1.avg], device=device),\n",
        "                            'val_recall3': torch.tensor([avg_recall3.avg], device=device), 'val_recall6': torch.tensor([avg_recall6.avg], device=device),\n",
        "                            'val_f1_1': torch.tensor([avg_f1_1.avg], device=device), 'val_f1_3': torch.tensor([avg_f1_3.avg], device=device),\n",
        "                            'val_f1_6': torch.tensor([avg_f1_6.avg], device=device)}\n",
        "\n",
        "        if self.aux_task and self.iter_epoch >= self.epoch_to_win:\n",
        "          tensorboard_logs['val_main_loss'] = torch.tensor([avg_main_loss.avg], device=device)\n",
        "          tensorboard_logs['val_win_loss'] = torch.tensor([avg_win_loss.avg], device=device)\n",
        "          tensorboard_logs['val_win_acc'] = torch.tensor([avg_acc.avg], device=device)\n",
        "          print('| loss_val {:5.6f} | main_loss_val {:5.6f} | win_loss_val {:5.6f} | precision_val {:5.6f} | map6_val {:5.6f} | acc_val {:5.6f}'.format(avg_loss.avg, avg_main_loss.avg, \n",
        "                                                                                                                                                        avg_win_loss.avg, avg_prec.avg, \n",
        "                                                                                                                                                        avg_map.avg, avg_acc.avg))\n",
        "        # else:\n",
        "        #   print('| loss_val {:5.6f} | precision1_val {:5.6f} | precision3_val {:5.6f} | precision6_val {:5.6f} | map1_val {:5.6f} | map3_val {:5.6f} | map6_val {:5.6f} | recall1 {:5.6f} | recall3 {:5.6f} | recall6 {:5.6f} | f1_1 {:5.6f} | f1_3 {:5.6f} | f1_6 {:5.6f}'.format(\n",
        "        #       avg_loss.avg, avg_prec1.avg, avg_prec3.avg, avg_prec.avg, avg_map1.avg, avg_map3.avg, avg_map.avg, avg_recall1.avg, avg_recall3.avg, avg_recall6.avg, avg_f1_1.avg, avg_f1_3.avg, avg_f1_6.avg))\n",
        "        \n",
        "        path_save_att = path_save_att_format.format(str(self.conf['index_split']), str(self.conf['exp']), str(self.iter_epoch))\n",
        "        path_save_list_metrics = path_save_list_metrics_format.format(str(self.conf['index_split']), str(self.conf['exp']), str(self.iter_epoch))\n",
        "        weights_items = {\n",
        "            'list_att_weights': list_att_weights,\n",
        "            'list_logits_items': list_logits_items\n",
        "        }\n",
        "\n",
        "        list_metrics = {\n",
        "            'list_prec1': list_prec1, \n",
        "            'list_prec3': list_prec3,\n",
        "            'list_prec6': list_prec6,\n",
        "            'list_recall1': list_recall1,\n",
        "            'list_recall3': list_recall3,\n",
        "            'list_recall6': list_recall6,\n",
        "            'list_map1': list_map1,\n",
        "            'list_map3': list_map3,\n",
        "            'list_map6': list_map6\n",
        "        }\n",
        "        save_att_weights(weights_items, path_save_att)\n",
        "        save_att_weights(list_metrics, path_save_list_metrics)\n",
        "        return {'avg_val_loss': avg_loss.avg,  'avg_val_prec': avg_prec.avg, 'val_map6': avg_map.avg,'progress_bar': tensorboard_logs,'log': tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # OPTIONAL\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        \n",
        "        return self.validation_end(outputs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # REQUIRED\n",
        "        # can return multiple optimizers and learning_rate schedulers\n",
        "        # (LBFGS it is automatically supported, no need for closure function)\n",
        "        if self.optim == 'adabound':\n",
        "          optimizer = adabound.AdaBound(self.model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "        else:\n",
        "          optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        return optimizer\n",
        "    \n",
        "    def train_dataset(self):\n",
        "\n",
        "      data = get_partition(self.index_split, list_trainset)\n",
        "      composed = transforms.Compose([RandomSort_Part(),\n",
        "                               RandomSort_Team()])\n",
        "      train_dataset = LolDataset(data, transform=composed)\n",
        "      return train_dataset\n",
        "\n",
        "    @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        \n",
        "        train_dataset = self.train_dataset()\n",
        "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        #data = list_testset[self.index_split]\n",
        "        data = get_partition(self.index_split, list_testset)\n",
        "        val_dataset = LolDataset(data)\n",
        "        return DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "    \n",
        "    @pl.data_loader\n",
        "    def test_dataloader(self):\n",
        "        # OPTIONAL\n",
        "        return self.val_dataloader()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyRfaqN8XvYi",
        "colab_type": "text"
      },
      "source": [
        "# Config file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Aa7VmxeYF7X",
        "colab_type": "text"
      },
      "source": [
        "This config establish the model hyperparameters like:\n",
        "\n",
        "1. index_split - num of the partition used to train.\n",
        "2. optim - optimizer (could be adam or adabound).\n",
        "3. batch_size - Batch size \n",
        "4. embeddings_size - model dim\n",
        "5. nhead - number of attention heads \n",
        "6. nlayers - number of encoder layers\n",
        "7. exp - experiment number\n",
        "8. epochs - number of epoch\n",
        "9. exp_name - experiment name in comet.ml\n",
        "10. alpha, beta - importance weights for losses\n",
        "11. win_task - enable the auxiliary task.\n",
        "12. learnable_team_emb - when it is True the team embedding is learnable \n",
        "otherwise it is static.  \n",
        "13. teacher_forcing - enable the teacher forcing for the auxiliary task.\n",
        "14. init_epoch - indicate the epoch when the second task start. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqm4Zi0_ha8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = {\n",
        "    'index_split': 0,\n",
        "    'optim': 'adam',\n",
        "    'seed': 1642,\n",
        "    'batch_size': 100,\n",
        "    'embeddings_size': 512,\n",
        "    'nhead': 2,\n",
        "    'nlayers': 1, \n",
        "    'nhid': 2048, \n",
        "    'dropout': 0.5,\n",
        "    'exp': 13,\n",
        "    'epochs': 10,\n",
        "    'exp_name': 'Main_tasks_rec_only_winners_final_prueba',\n",
        "    'win_task': False,\n",
        "    'alpha': 1,\n",
        "    'beta': 1,\n",
        "    'learnable_team_emb': True,\n",
        "    'teacher_forcing': False,\n",
        "    'init_epoch': 2\n",
        "}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVtKoVTcYDS1",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation executor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIBfoXNe1TOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833,
          "referenced_widgets": [
            "a705d3b71b5d4fb587b3bb1fb38161fa",
            "abfceea38af9444b8da09122eb0c867d",
            "0dfe065d9c0c468389f75dd74a9a11ac",
            "741dabfff47941f3b290d4ad4cb6be12",
            "503e7ca9191948a4bcf6cc64a8862820",
            "249092a9ce0940218d291fe75cf35f3e",
            "828e6e2c56f3456cb016bf7c8b701ba8",
            "7540c343bf9f461a84c157f84d529cd9",
            "c43e576730a940c28fa78d49e95e7165",
            "b3d8f9b86d0d47ae8c51b8f2eb202aab",
            "82170aabdef246edbf668bb1cdf4a5e3",
            "43a58bf6d9ab454095f8f5f30f10cdca",
            "f80b50d773b240a091c6c3bdf7961924",
            "f8a352088b8d4ed896bf8a206ecc024e",
            "2e151daf92e84c369ee90e8ded7a24f2",
            "5b0b2240357743c4b8285ce6017638c4"
          ]
        },
        "outputId": "3c6e7509-69b7-4562-e993-c1012e1c94dc"
      },
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "path_save = '/content/gdrive/My Drive/Proyecto_RecSys/split/{}/exp_recsys/{}/checkpoints/'.format(str(conf['index_split']), str(conf['exp']))\n",
        "path_save_att_format = '/content/gdrive/My Drive/Proyecto_RecSys/split/{}/exp_recsys/{}/checkpoints/att_weights_{}.pkl'\n",
        "path_save_list_metrics_format = '/content/gdrive/My Drive/Proyecto_RecSys/split/{}/exp_recsys/{}/checkpoints/list_metrics_{}.pkl'\n",
        "\n",
        "model = LolRecAttModel(conf)\n",
        "\n",
        "checkpoint_callback = get_checkpointer(path_save,'avg_val_prec')\n",
        "\n",
        "\n",
        "comet_logger = CometLogger(\n",
        "    experiment_name=conf['exp_name'],\n",
        "    api_key = 'YOUR_KEY',\n",
        "    project_name=\"YOUR_PROJECT_NAME\",\n",
        "    workspace = 'YOUR_WORKSPACE'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    gpus=[0],\n",
        "    distributed_backend='dp',\n",
        "    logger=comet_logger,\n",
        "    max_epochs=conf['epochs'],\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    show_progress_bar=False,\n",
        "    gradient_clip_val=0.5\n",
        ")\n",
        "\n",
        "trainer.fit(model)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:22: UserWarning: Checkpoint directory /content/gdrive/My Drive/Proyecto_RecSys/split/0/exp_recsys/13/checkpoints/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "CometLogger will be initialized in online mode\n",
            "COMET INFO: ----------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary:\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     url: https://www.comet.ml/afvilla/lolnet/5094e7cd80244d62bac9be446cbfeb0b\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     sys.cpu.percent.01 [4]       : (1.0, 12.3)\n",
            "COMET INFO:     sys.cpu.percent.02 [4]       : (1.0, 12.9)\n",
            "COMET INFO:     sys.cpu.percent.03 [4]       : (0.9, 12.5)\n",
            "COMET INFO:     sys.cpu.percent.04 [4]       : (1.0, 12.9)\n",
            "COMET INFO:     sys.cpu.percent.avg [4]      : (0.975, 12.65)\n",
            "COMET INFO:     sys.gpu.0.free_memory [4]    : (17061249024.0, 17061249024.0)\n",
            "COMET INFO:     sys.gpu.0.gpu_utilization [4]: (0.0, 0.0)\n",
            "COMET INFO:     sys.gpu.0.total_memory       : (17071734784.0, 17071734784.0)\n",
            "COMET INFO:     sys.gpu.0.used_memory [4]    : (10485760.0, 10485760.0)\n",
            "COMET INFO:     sys.ram.total [4]            : (27393740800.0, 27393740800.0)\n",
            "COMET INFO:     sys.ram.used [4]             : (7792205824.0, 7797219328.0)\n",
            "COMET INFO:   Other [count]:\n",
            "COMET INFO:     Name: Main_tasks_rec_only_winners_final_prueba\n",
            "COMET INFO: ----------------------------\n",
            "COMET INFO: old comet version (3.0.2) detected. current: 3.1.14 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/afvilla/lolnet/8dcde95206ec45daac4cc6657844b03d\n",
            "\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type                      | Params\n",
            "-------------------------------------------------------\n",
            "0 | model    | TransformerLolRecommender | 3 M   \n",
            "1 | loss     | BCEWithLogitsLoss         | 0     \n",
            "2 | loss_aux | CrossEntropyLoss          | 0     \n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:22: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a705d3b71b5d4fb587b3bb1fb38161fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:22: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c43e576730a940c28fa78d49e95e7165",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
